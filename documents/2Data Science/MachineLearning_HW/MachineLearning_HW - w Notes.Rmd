---
title: "Machine Learning"
author: "Adam F Clark"
output: html_document
---

### Executive Summary
In this paper we discuss one method of designing an algorithm to predict the manner in which a person wearing a specific type of accelerometers is performing barbell lifts.  The model-training data includes the accelerometer readings from six persons who performed the barbell lifts; each with the proper form and four improper forms.  This paper shows how the prediction model was built, cross-validated and provides estimates of the errors.

### 1.0 Exploratory Analysis and Basic Data Summary
```{r, DeclareLibraries, message=FALSE, warning=FALSE, results="Hide", echo=FALSE}
library(dplyr); library(ggplot2); library(caret); library(randomForest)
```

This data set contains 19,622 observations of 159 independent variables, though not all observations occur at the same temporal frequency.  An examination of the data shows that there are many variables with very sparse data.  For this simple homework project, these extremely sparse variables are ignored.  However, it is understood that in practice, much more care should be exerted on discovering the meaning of these variables and the reason for their sparsity.  There are likely more elegant methods for handling them than shown here.

Furthermore, it causes some consternation to not take the time variables into better account.  Perhaps the accelerometer reading at any specific time is not as important as the full motion of the move over time (indicated by many rows in the data set).  It seems worthwhile to create variables that try to capture these time-based trends.  Since this algorithm will be tested on single-row records, this extension cannot be done here and is left as an exercise to the reader.

Furthermore, the fields that show when the actions took place were also removed.    Their predictive capabilities may be very good on the original dataset, but would most certainly prove inadequate on future data sets.

```{r, Exploratory1}
# setwd(".//2Data Science//data")
nonSparseList <- c(6:11,37:49,60:68,84:86,102,113:124,140,151:160)
pmlRawData <- read.csv("pml-training.csv")
pmlData <- pmlRawData[,nonSparseList]

#pmlData[,2:54] <- as.numeric(pmlData[,2:54])
#pmlData <- sapply(pmlData[,2:54],function(x) x <- as.numeric(x))


pmlRawTest <- read.csv("pml-training.csv")
pmlTest <- pmlRawTest[,nonSparseList]

set.seed(12)
inTest <- createDataPartition(y=pmlData$classe,p=0.3,list=FALSE)
myTest <- pmlData[inTest,]
myTrain <- pmlData[-inTest,]

```

Since the data set is sufficiently large, 30% of the data is randomly selected and set aside for a testing set, and thus 70% of the records are used for training.

### 2.0 Simple Random Forest Approach

The response variable is discrete (Class A, B, C, D or E) so it seems simpler to use a classification tree-style approach as opposed to regression.  So to start, we will use a simple Random Forest approach.  While the final model's worth will be predicted using the testing set (described above), a 3-fold cross-validation is done by the Random Forest algorithm to help train the model.  More "folds" may provide a better model, however the smaller number was chosen for computation-time purposes.

```{r, RandomForestModels1}
#cat("Starts at ",hour(now()),minute(now()),second(now()))

#myFitCt <- train(classe~.,
#               data=myTrain,
#               method="rf",
#               cv.fold=3,
#               prox=TRUE)

myFit <- randomForest(myTrain[,-55],
                myTrain$classe,
                cv.fold=3,
                na.action = na.omit,
                imporance=TRUE,
                proximity=TRUE)

myFit

```

The resulting Random Forest model has an estimated error rate of less than 1%.  The confusion matrix shows slightly better prediction of a properly-performed barbell lift than it does for the common improperly-performed lifts.

```{r, RandomForestModels2}
myVarImp <- varImp(myFit, scale = FALSE)
```

The most "important" variable, as calculated by the model, is "num window".  I searched for some explanation of this variable on the GroupWare website, but never found a good description.  Thus I can't provide any hypothesis as to why this variable may hold so much prediction power (and ultimately lowers my personal confidence in the model).  The next most important variables are the "roll belt", "yaw belt", and "pitch forearm".  These variables partially explain the gross movements of the belt and forearm.  If this model proves to be useful, then one might assume the arm accelerometers are not needed to predict future barbell quality.

What is the error rate?

### Conclusion

This algorithm is awesome.